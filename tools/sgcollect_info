#!/usr/bin/env python3

"""
Copyright 2016-Present Couchbase, Inc.

Use of this software is governed by the Business Source License included in
the file licenses/BSL-Couchbase.txt.  As of the Change Date specified in that
file, in accordance with the Business Source License, use of this software will
be governed by the Apache License, Version 2.0, included in the file
licenses/APL2.txt.
"""

# -*- python -*-
import base64
import glob
import json
import optparse
import os
import platform
import re
import ssl
import subprocess
import sys
import urllib.error
import urllib.parse
import urllib.request
import uuid
from sys import platform as _platform

import password_remover
from tasks import AllOsTask
from tasks import CbcollectInfoOptions
from tasks import TaskRunner
from tasks import add_file_task
from tasks import add_gzip_file_task
from tasks import do_upload_and_exit
from tasks import dump_utilities
from tasks import flatten
from tasks import generate_upload_url
from tasks import log
from tasks import make_curl_task
from tasks import make_os_tasks
from tasks import setup_stdin_watcher

try:
    # Don't validate HTTPS by default.
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    # Running an older version of Python which won't validate HTTPS anyway.
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# Collects the following info from Sync Gateway
#
# - System Stats (top, netstat, etc)
# - Sync Gateway logs
# - Expvar Json
# - pprof files (profiling / memory)
# - Startup and running SG config
#
# See https://github.com/couchbase/sync_gateway/issues/1640
#
# Python version compatibility:
#
# Until the libc / centos6 issues are resolved, this should remain python 2.6 compatible.
# One common incompatibility is the formatting syntax, as discussed here: http://bit.ly/2rIH8wg
USAGE = """usage: %prog [options] output_file.zip

- Linux/Windows/OSX:
    %prog output_file.zip
    %prog -v output_file.zip"""

mydir = os.path.dirname(sys.argv[0])


def create_option_parser():
    parser = optparse.OptionParser(usage=USAGE, option_class=CbcollectInfoOptions)
    parser.add_option("-r", dest="root",
                      help="root directory - defaults to %s" % (mydir + "/.."),
                      default=os.path.abspath(os.path.join(mydir, "..")))
    parser.add_option("-v", dest="verbosity", help="increase verbosity level",
                      action="count", default=0)
    parser.add_option("-p", dest="product_only", help="gather only product related information",
                      action="store_true", default=False)
    parser.add_option("-d", action="callback", callback=dump_utilities,
                      help="dump a list of commands that sgcollect_info needs")
    parser.add_option("--watch-stdin", dest="watch_stdin",
                      action="store_true", default=False,
                      help=optparse.SUPPRESS_HELP)
    parser.add_option("--log-redaction-level", dest="redact_level",
                      default="none",
                      help="redaction level for the logs collected, none and partial supported (default is none)")
    parser.add_option("--log-redaction-salt", dest="salt_value",
                      default=str(uuid.uuid4()),
                      help="Is used to salt the hashing of tagged data, \
                            defaults to random uuid. If input by user it should \
                            be provided along with --log-redaction-level option")
    parser.add_option("--just-upload-into", dest="just_upload_into",
                      help=optparse.SUPPRESS_HELP)
    parser.add_option("--upload-host", dest="upload_host",
                      help="if specified, gathers diagnostics and uploads it to the specified host,"
                           " e.g 'https://uploads.couchbase.com'")
    parser.add_option("--customer", dest="upload_customer",
                      help="used in conjunction with '--upload-host' and '--ticket', "
                           "specifies the customer name for the upload")
    parser.add_option("--ticket", dest="upload_ticket", type='ticket',
                      help="used in conjunction with '--upload-host' and '--customer',"
                           " specifies the support ticket number for the upload."
                           " e.g 1234 (must be numeric), contact Couchbase Support to open a new"
                           "ticket if you do not already have one.  For more info, see"
                           "http://www.couchbase.com/wiki/display/couchbase/Working+with+the+Couchbase+Technical+Support+Team")
    parser.add_option("--sync-gateway-url", dest="sync_gateway_url",
                      help="Sync Gateway admin port URL, eg, http://localhost:4985")
    parser.add_option("--sync-gateway-config", dest="sync_gateway_config",
                      help="path to Sync Gateway config.  By default will try to discover via expvars")
    parser.add_option("--sync-gateway-executable", dest="sync_gateway_executable",
                      help="path to Sync Gateway executable.  By default will try to discover via expvars")
    parser.add_option("--sync-gateway-username", dest="sync_gateway_username",
                      help="Sync Gateway Admin API username ")
    parser.add_option("--sync-gateway-password", dest="sync_gateway_password",
                      help="Sync Gateway Admin API password ")
    parser.add_option("--upload-proxy", dest="upload_proxy", default="",
                      help="specifies proxy for upload")
    parser.add_option("--tmp-dir", dest="tmp_dir", default=None,
                      help="set the temp dir used while processing collected data. Overrides the TMPDIR env variable if set")
    return parser


def expvar_url(sg_url):

    return '{0}/_expvar'.format(sg_url)


def make_http_client_pprof_tasks(sg_url, sg_username, sg_password):

    """
    These tasks use the python http client to collect the raw pprof data, which can later
    be rendered into something human readable
    """
    profile_types = [
        "profile",
        "heap",
        "goroutine",
        "block",
        "mutex",
    ]

    base_pprof_url = "{0}/_debug/pprof".format(sg_url)

    pprof_tasks = []
    for profile_type in profile_types:
        sg_pprof_url = "{0}/{1}".format(base_pprof_url, profile_type)
        clean_task = make_curl_task(name="Collect {0} pprof via http client".format(profile_type),
                                    user=sg_username,
                                    password=sg_password,
                                    url=sg_pprof_url,
                                    log_file="pprof_{0}.pb.gz".format(profile_type))
        clean_task.no_header = True
        pprof_tasks.append(clean_task)

    return pprof_tasks


def to_lower_case_keys_dict(original_dict):
    result = {}
    for k, v in list(original_dict.items()):
        result[k.lower()] = v
    return result


def extract_element_from_config(element, config):
    """ The config returned from /_config may not be fully formed json
        due to the fact that the sync function is inside backticks (`)
        Therefore this method grabs an element from the config after
        removing the sync function
    """

    sync_regex = r'"Sync":(`.*`)'
    config = re.sub(sync_regex, '"Sync":""', config)
    try:
        # convert dictionary keys to lower case
        original_dict = json.loads(config)
        lower_case_keys_dict = to_lower_case_keys_dict(original_dict)

        # lookup key after converting element name to lower case
        return lower_case_keys_dict[element.lower()]
    except (ValueError, KeyError):
        # If we can't deserialize the json or find the key then return nothing
        return


def extract_element_from_default_logging_config(element, config):
    # extracts a property from nested logging object
        try:
            logging_config = extract_element_from_config('Logging', config)
            if logging_config:
                default_logging_config = extract_element_from_config('default', json.dumps(logging_config))
                if default_logging_config:
                    guessed_log_path = extract_element_from_config(element, json.dumps(default_logging_config))
                    if guessed_log_path:
                        return guessed_log_path
            return
        except (ValueError, KeyError):
            # If we can't deserialize the json or find the key then return nothing
            return


def extract_element_from_logging_config(element, config):
    # extracts a property from nested logging object
        try:
            logging_config = extract_element_from_config('logging', config)
            if logging_config:
                guessed_log_path = extract_element_from_config(element, json.dumps(logging_config))
                if guessed_log_path:
                    return guessed_log_path
            return
        except (ValueError, KeyError):
            # If we can't deserialize the json or find the key then return nothing
            return

def urlopen_with_basic_auth(url, username, password):
    if username and len(username) > 0:
        # Add basic auth header
        request = urllib.request.Request(url)
        base64string = base64.b64encode(bytes('%s:%s' % (username, password),'utf-8'))
        request.add_header("Authorization", "Basic %s" % base64string.decode('utf-8'))
        return urllib.request.urlopen(request)
    else:
        return urllib.request.urlopen(url)

def make_collect_logs_tasks(zip_dir, sg_url, sg_config_file_path, sg_username, sg_password, salt, should_redact):

    sg_log_files = {
        "sg_error.log": "sg_error.log",
        "sg_warn.log": "sg_warn.log",
        "sg_info.log": "sg_info.log",
        "sg_debug.log": "sg_debug.log",
        "sg_stats.log": "sg_stats.log",
        "sync_gateway_access.log": "sync_gateway_access.log",
        "sync_gateway_error.log": "sync_gateway_error.log",
    }

    os_home_dirs = [
        "/home/sync_gateway/logs",
        "/var/log/sync_gateway",
        "/Users/sync_gateway/logs",                                                     # OSX sync gateway
        R'C:\Program Files (x86)\Couchbase\var\lib\couchbase\logs',                     # Windows (Pre-2.0)
        R'C:\Program Files\Couchbase\var\lib\couchbase\logs',                           # Windows (Post-2.0)
        R'C:\Program Files\Couchbase\Sync Gateway\var\lib\couchbase\logs'               # Windows (Post-2.1) sync gateway
    ]
    # Try to find user-specified log path
    if sg_url:
        config_url = "{0}/_config".format(sg_url)
        try:
            response = urlopen_with_basic_auth(config_url, sg_username, sg_password)
        except urllib.error.URLError:
            print("Failed to load SG config from running SG.")
            config_str = ""
        else:
            config_str = response.read().decode('utf-8')
    else:
        config_str = ""
    
    # If SG isn't running, load the bootstrap config - it'll have the logging config in it
    if config_str == "" and sg_config_file_path is not None and sg_config_file_path != "":
        try:
            with open(sg_config_file_path) as fd:
                print("Loading SG config from path on disk.")
                config_str = fd.read()
        except Exception as e:
            print("Failed to load SG config from disk: {0}".format(e))

    # Find log file path from old style top level config
    guessed_log_path = extract_element_from_config('LogFilePath', config_str)
    if guessed_log_path and os.path.isfile(guessed_log_path):
        # If the specified log path is a file, add filename to standard SG log files list
        # and parent directory path of the file to standard SG log directories list to
        # eventually look for all permutations of SG log files and directories.
        os_home_dirs.append(os.path.dirname(guessed_log_path))
        sg_log_files[os.path.basename(guessed_log_path)] = os.path.basename(guessed_log_path)
    elif guessed_log_path and os.path.isdir(guessed_log_path):
        # If the specified log path is a directory, add that path to the standard
        # SG log directories list to eventually look for the standard SG log files.
        os_home_dirs.append(guessed_log_path)

    # Keep a dictionary of log file paths we've added, to avoid adding duplicates
    sg_log_file_paths = {}

    sg_tasks = []

    def lookup_std_log_files(files, dirs):
        for dir in dirs:
            for file in files:
                name, ext = os.path.splitext(file)
                # Collect active and rotated log files from the default log locations.
                pattern_rotated = os.path.join(dir, "{0}*{1}".format(name, ext))
                for std_log_file in glob.glob(pattern_rotated):
                    if std_log_file not in sg_log_file_paths:
                        sg_tasks.append(add_file_task(sourcefile_path=std_log_file))
                        sg_log_file_paths[std_log_file] = std_log_file

                # Collect archived log files from the default log locations.
                pattern_archived = os.path.join(dir, "{0}*{1}.gz".format(name, ext))
                for std_log_file in glob.glob(pattern_archived):
                    if std_log_file not in sg_log_file_paths:
                        if should_redact:
                            task = add_gzip_file_task(sourcefile_path=std_log_file, salt=salt)
                            sg_tasks.append(task)
                        else:
                            task = add_file_task(sourcefile_path=std_log_file)
                            task.no_header = True
                            sg_tasks.append(task)

    # Lookup each standard SG log files in each standard SG log directories.
    lookup_std_log_files(sg_log_files, os_home_dirs)

    # Find log file path from logging.["default"] style config
    # When the log file path from the default style config is a directory, we need to look for all
    # standard SG log files inside the directory including rotated log files. But that handling is
    # not included here, it relies on the fall through to the 2.1 style handling to pick up those
    # log files that were written to the directory path instead; SG exposes the log_file_path as
    # the parent directory of the file specified against LogFilePath through /_config endpoint.
    guessed_logging_path = extract_element_from_default_logging_config('LogFilePath', config_str)
    if guessed_logging_path and os.path.isfile(guessed_logging_path):
        # Get the parent directory and the log file name
        log_file_parent_dir = os.path.abspath(os.path.join(guessed_logging_path, os.pardir))
        log_file_name = os.path.basename(guessed_logging_path)
        name, ext = os.path.splitext(log_file_name)

        # Lookup SG log files inside the parent directory, including rotated log files.
        rotated_logs_pattern = os.path.join(log_file_parent_dir, "{0}*{1}".format(name, ext))
        for log_file_item_name in glob.iglob(rotated_logs_pattern):
            log_file_item_path = os.path.join(log_file_parent_dir, log_file_item_name)
            # As long as a task that monitors this log file path has not already been added, add a new task
            if log_file_item_path not in sg_log_file_paths:
                print('Capturing rotated log file {0}'.format(log_file_item_path))
                task = add_file_task(sourcefile_path=log_file_item_path)
                sg_tasks.append(task)
                # Track which log file paths have been added so far.
                sg_log_file_paths[log_file_item_path] = log_file_item_path

        # Lookup standard SG log files inside the parent directory.
        lookup_std_log_files(sg_log_files, [log_file_parent_dir])

    # Find log file path from SGW 2.1 style logging config
    guessed_log_file_path = extract_element_from_logging_config('log_file_path', config_str)
    if guessed_log_file_path:
        log_file_path = os.path.abspath(guessed_log_file_path)

        for log_file_name in sg_log_files:
            # iterate over all log files, including those with a rotation timestamp
            # e.g: sg_info-2018-12-31T13-33-41.055.log
            name, ext = os.path.splitext(log_file_name)
            log_file_pattern = "{0}*{1}".format(name, ext)
            rotated_logs_pattern = os.path.join(log_file_path, log_file_pattern)

            for log_file_item_name in glob.iglob(rotated_logs_pattern):
                log_file_item_path = os.path.join(log_file_path, log_file_item_name)
                # As long as a task that monitors this log file path has not already been added, add a new task
                if log_file_item_path not in sg_log_file_paths:
                    print('Capturing rotated log file {0}'.format(log_file_item_path))
                    task = add_file_task(sourcefile_path=log_file_item_path)
                    sg_tasks.append(task)

                    # Track which log file paths have been added so far
                    sg_log_file_paths[log_file_item_path] = log_file_item_path

            # try gzipped logs too
            # e.g: sg_info-2018-12-31T13-33-41.055.log.gz
            log_file_pattern = "{0}*{1}.gz".format(name, ext)
            rotated_logs_pattern = os.path.join(log_file_path, log_file_pattern)

            for log_file_item_name in glob.iglob(rotated_logs_pattern):
                log_file_item_path = os.path.join(log_file_path, log_file_item_name)
                # As long as a task that monitors this log file path has not already been added, add a new task
                if log_file_item_path not in sg_log_file_paths:
                    print('Capturing compressed rotated log file {0}'.format(log_file_item_path))
                    # If we're redacting a gzipped log file, we'll need to extract, redact and recompress it.
                    # If we're not redacting, we can skip extraction entirely, and use the existing .gz log file.
                    if should_redact:
                        task = add_gzip_file_task(sourcefile_path=log_file_item_path, salt=salt)
                        sg_tasks.append(task)
                    else:
                        task = add_file_task(sourcefile_path=log_file_item_path)
                        task.no_header = True
                        sg_tasks.append(task)

                    # Track which log file paths have been added so far
                    sg_log_file_paths[log_file_item_path] = log_file_item_path

    return sg_tasks


def get_db_list(sg_url, sg_username, sg_password):

    # build url to _all_dbs
    all_dbs_url = "{0}/_all_dbs".format(sg_url)
    data = []

    # get content and parse into json
    try:
        response = urlopen_with_basic_auth(all_dbs_url, sg_username, sg_password)
        data = json.load(response)
    except urllib.error.URLError as e:
        print("WARNING: Unable to connect to Sync Gateway: {0}".format(e))

    # return list of dbs
    return data

# Startup config
#   Commandline args (covered in expvars, IIRC)
#   json file.
# Running config
#   Server config
#   Each DB config
def make_config_tasks(zip_dir, sg_config_path, sg_url, sg_username, sg_password, should_redact):

    collect_config_tasks = []

    # Here are the "usual suspects" to probe for finding the static config
    sg_config_files = [
        "/home/sync_gateway/sync_gateway.json",                          # linux sync gateway
        "/opt/sync_gateway/etc/sync_gateway.json",                       # amazon linux AMI sync gateway
        "/Users/sync_gateway/sync_gateway.json"                          # OSX sync gateway
        R'C:\Program Files (x86)\Couchbase\serviceconfig.json'           # Windows (Pre-2.0) sync gateway
        R'C:\Program Files\Couchbase\Sync Gateway\serviceconfig.json'    # Windows (Post-2.0) sync gateway
    ]
    sg_config_files = [x for x in sg_config_files if os.path.exists(x)]

    # If a config path was discovered from the expvars, or passed in via the user, add that in the
    # list of files to probe
    if sg_config_path is not None:
        sg_config_files.append(sg_config_path)

    # Tag user data before redaction, if redact_level is set
    server_config_postprocessors = [password_remover.remove_passwords]
    db_config_postprocessors = [password_remover.remove_passwords]
    if should_redact:
        server_config_postprocessors.append(password_remover.tag_userdata_in_server_config)
        db_config_postprocessors.append(password_remover.tag_userdata_in_db_config)

    # Get static server config
    for sg_config_file in sg_config_files:
        task = add_file_task(sourcefile_path=sg_config_file, content_postprocessors=server_config_postprocessors)
        collect_config_tasks.append(task)

    # Get server config
    server_config_url = "{0}/_config".format(sg_url)

    config_task = make_curl_task(name="Collect server config",
                                 user=sg_username,
                                 password=sg_password,
                                 url=server_config_url,
                                 log_file="sync_gateway.log",
                                 content_postprocessors=server_config_postprocessors)
    collect_config_tasks.append(config_task)

    # Get server config with runtime defaults and runtime dbconfigs
    server_runtime_config_url = "{0}/_config?include_runtime=true".format(sg_url)
    runtime_config_task = make_curl_task(name="Collect runtime config",
                                 user=sg_username,
                                 password=sg_password,
                                 url=server_runtime_config_url,
                                 log_file="sync_gateway.log",
                                 content_postprocessors=server_config_postprocessors)
    collect_config_tasks.append(runtime_config_task)
    
    # Get persisted dbconfigs
    dbs = get_db_list(sg_url, sg_username, sg_password)
    for db in dbs:
        db_config_url = "{0}/{1}/_config".format(sg_url, db)
        db_config_task = make_curl_task(name="Collect {0} database config".format(db),
                                 user=sg_username,
                                 password=sg_password,
                                 url=db_config_url,
                                 log_file="sync_gateway.log",
                                 content_postprocessors=db_config_postprocessors)
        collect_config_tasks.append(db_config_task)
    
    return collect_config_tasks


def get_config_path_from_cmdline(cmdline_args):

    for cmdline_arg in cmdline_args:
        # if it has .json in the path, assume it's a config file.
        # ignore any config files that are URL's for now, since
        # they won't be handled correctly.
        if ".json" in cmdline_arg and "http" not in cmdline_arg:
            return cmdline_arg
    return None


def get_paths_from_expvars(sg_url, sg_username, sg_password):

    data = None
    sg_binary_path = None
    sg_config_path = None

    # get content and parse into json
    if sg_url:
        try:
            response = urlopen_with_basic_auth(expvar_url(sg_url), sg_username, sg_password)
            # response = urllib.request.urlopen(expvar_url(sg_url))
            data = json.load(response)
        except urllib.error.URLError as e:
            print("WARNING: Unable to connect to Sync Gateway: {0}".format(e))

    if data is not None and "cmdline" in data:
        cmdline_args = data["cmdline"]
        if len(cmdline_args) == 0:
            return (sg_binary_path, sg_config_path)
        sg_binary_path = cmdline_args[0]
        if len(cmdline_args) > 1:
            try:
                sg_config_path = get_absolute_path(get_config_path_from_cmdline(cmdline_args[1:]))
            except Exception as e:
                print("Exception trying to get absolute sync gateway path from expvars: {0}".format(e))
                sg_config_path = get_config_path_from_cmdline(cmdline_args[1:])

    return (sg_binary_path, sg_config_path)


def get_absolute_path(relative_path):
    sync_gateway_cwd = ''
    try:
        if _platform.startswith("linux"):
            sync_gateway_pid = subprocess.check_output(['pgrep', 'sync_gateway']).split()[0]
            sync_gateway_cwd = subprocess.check_output(['readlink', '-e', '/proc/{0}/cwd'.format(sync_gateway_pid)]).strip('\n')
    except subprocess.CalledProcessError:
        pass

    return os.path.join(sync_gateway_cwd, relative_path)


def make_download_expvars_task(sg_url, sg_username, sg_password):

    task = make_curl_task(
        name="download_sg_expvars",
        user=sg_username,
        password=sg_password,
        url=expvar_url(sg_url),
        log_file="expvars.json"
    )

    task.no_header = True

    return task


def make_sg_tasks(zip_dir, sg_url, sg_username, sg_password, sync_gateway_config_path_option, sync_gateway_executable_path, should_redact, salt):

    # Get path to sg binary (reliable) and config (not reliable)
    sg_binary_path, sg_config_path = get_paths_from_expvars(sg_url, sg_username, sg_password)
    print("Discovered from expvars: sg_binary_path={0} sg_config_path={1}".format(sg_binary_path, sg_config_path))

    # If user passed in a specific path to the SG binary, then use it
    if sync_gateway_executable_path is not None and len(sync_gateway_executable_path) > 0:
        if not os.path.exists(sync_gateway_executable_path):
            raise Exception("Path to sync gateway executable passed in does not exist: {0}".format(sync_gateway_executable_path))
        sg_binary_path = sync_gateway_executable_path
    
    if sg_config_path is None and sync_gateway_config_path_option is not None and len(sync_gateway_config_path_option) > 0:
        sg_config_path = sync_gateway_config_path_option

    # Collect logs
    collect_logs_tasks = make_collect_logs_tasks(zip_dir, sg_url, sg_config_path, sg_username, sg_password, salt, should_redact)

    py_expvar_task = make_download_expvars_task(sg_url, sg_username, sg_password)

    # If the user passed in a valid config path, then use that rather than what's in the expvars
    if sync_gateway_config_path_option is not None and len(sync_gateway_config_path_option) > 0 and os.path.exists(sync_gateway_config_path_option):
        sg_config_path = sync_gateway_config_path_option

    http_client_pprof_tasks = make_http_client_pprof_tasks(sg_url, sg_username, sg_password)

    # Add a task to collect Sync Gateway config
    config_tasks = make_config_tasks(zip_dir, sg_config_path, sg_url, sg_username, sg_password, should_redact)

    # Curl the /_status
    status_tasks = make_curl_task(name="Collect server status",
                                  user=sg_username,
                                  password=sg_password,
                                  url="{0}/_status".format(sg_url),
                                  log_file="sync_gateway.log",
                                  content_postprocessors=[password_remover.pretty_print_json])

    # Combine all tasks into flattened list
    sg_tasks = flatten(
        [
            collect_logs_tasks,
            py_expvar_task,
            http_client_pprof_tasks,
            config_tasks,
            status_tasks,
        ]
    )

    return sg_tasks


def discover_sg_binary_path(options, sg_url, sg_username, sg_password):

    sg_bin_dirs = [
        "/opt/couchbase-sync-gateway/bin/sync_gateway",                      # Linux + OSX
        R'C:\Program Files (x86)\Couchbase\sync_gateway.exe',                # Windows (Pre-2.0)
        R'C:\Program Files\Couchbase\Sync Gateway\sync_gateway.exe',         # Windows (Post-2.0)
    ]

    for sg_binary_path_candidate in sg_bin_dirs:
        if os.path.exists(sg_binary_path_candidate):
            return sg_binary_path_candidate

    sg_binary_path, _ = get_paths_from_expvars(sg_url, sg_username, sg_password)

    if options.sync_gateway_executable is not None and len(options.sync_gateway_executable) > 0:
        if not os.path.exists(options.sync_gateway_executable):
            raise Exception(
                "Path to sync gateway executable passed in does not exist: {0}".format(options.sync_gateway_executable))
        return sg_binary_path

    # fallback to whatever was specified in options
    return options.sync_gateway_executable


def main():

    # ask all tools to use C locale (MB-12050)
    os.environ['LANG'] = 'C'
    os.environ['LC_ALL'] = 'C'

    # Workaround MB-8239: erl script fails in OSX as it is unable to find COUCHBASE_TOP
    if platform.system() == 'Darwin':
        os.environ["COUCHBASE_TOP"] = os.path.abspath(os.path.join(mydir, ".."))

    # Parse command line options
    parser = create_option_parser()
    options, args = parser.parse_args()

    # Validate args
    if len(args) != 1:
        parser.error("incorrect number of arguments. Expecting filename to collect diagnostics into")

    # Setup stdin watcher if this option was passed
    if options.watch_stdin:
        setup_stdin_watcher()

    sg_url = options.sync_gateway_url
    sg_username = options.sync_gateway_username
    sg_password = options.sync_gateway_password

    if not sg_url or "://" not in sg_url:
        if not sg_url:
            root_url = "127.0.0.1:4985"
        else:
            root_url = sg_url
        sg_url_http = "http://" + root_url
        print("Trying Sync Gateway URL: {0}".format(sg_url_http))

        # Set sg_url to sg_url_http at this point
        # If we're unable to determine which URL to use this is our best
        # attempt. Avoids having this is 'None' later
        sg_url = sg_url_http

        try:
            response = urlopen_with_basic_auth(sg_url_http, sg_username, sg_password)
            json.load(response)
        except Exception as e:
            print("Failed to communicate with: {} {}".format(sg_url_http, e))
            sg_url_https = "https://" + root_url
            print("Trying Sync Gateway URL: {0}".format(sg_url_https))
            try:
                response = urlopen_with_basic_auth(sg_url_https, sg_username, sg_password)
                json.load(response)
            except Exception as e:
                print("Failed to communicate with Sync Gateway using url {}. "
                      "Check that Sync Gateway is running and reachable. "
                      "Will attempt to continue anyway.".format(e))
            else:
                sg_url = sg_url_https

    # Build path to zip directory, make sure it exists
    zip_filename = args[0]
    if zip_filename[-4:] != '.zip':
        zip_filename = zip_filename + '.zip'
    zip_dir = os.path.dirname(os.path.abspath(zip_filename))
    if not os.access(zip_dir, os.W_OK | os.X_OK):
        print("do not have write access to the directory %s" % (zip_dir))
        sys.exit(1)

    if options.redact_level != "none" and options.redact_level != "partial":
        parser.error("Invalid redaction level. Only 'none' and 'partial' are supported.")

    upload_url = ""
    should_redact = False
    if options.redact_level != "none":
        should_redact = True

        # Generate the s3 URL where zip files will be updated
        redact_zip_file = zip_filename[:-4] + "-redacted" + zip_filename[-4:]
        upload_url = generate_upload_url(parser, options, redact_zip_file)
    else:
        upload_url = generate_upload_url(parser, options, zip_filename)

    # Linux
    if os.name == 'posix':

        path = [
            mydir,
            '/opt/couchbase/bin',
            os.environ['PATH'],
            '/bin',
            '/sbin',
            '/usr/bin',
            '/usr/sbin'
        ]
        os.environ['PATH'] = ':'.join(path)

        library_path = [
            os.path.join(options.root, 'lib')
        ]

        current_library_path = os.environ.get('LD_LIBRARY_PATH')
        if current_library_path is not None:
            library_path.append(current_library_path)

        os.environ['LD_LIBRARY_PATH'] = ':'.join(library_path)

    # Windows
    elif os.name == 'nt':

        path = [
            mydir,
            os.environ['PATH']
        ]
        os.environ['PATH'] = ';'.join(path)

    # If user asked to just upload, then upload and exit
    if options.just_upload_into is not None:
        do_upload_and_exit(args[0], options.just_upload_into, options.upload_proxy)

    # Create a TaskRunner and run all of the OS tasks (collect top, netstat, etc)
    # The output of the tasks will go directly into couchbase.log
    runner = TaskRunner(verbosity=options.verbosity,
                        default_name="sync_gateway.log",
                        tmp_dir=options.tmp_dir)

    if not options.product_only:
        for task in make_os_tasks(["sync_gateway"]):
            runner.run(task)

    # Output the Python version if verbosity was enabled
    if options.verbosity:
        log("Python version: %s" % sys.version)

    # Find path to sg binary
    sg_binary_path = discover_sg_binary_path(options, sg_url, sg_username, sg_password)

    # Run SG specific tasks
    for task in make_sg_tasks(zip_dir, sg_url, sg_username, sg_password, options.sync_gateway_config, options.sync_gateway_executable, should_redact, options.salt_value):
        runner.run(task)

    if sg_binary_path is not None and sg_binary_path != "" and os.path.exists(sg_binary_path):
        runner.collect_file(sg_binary_path)
    else:
        print("WARNING: unable to find Sync Gateway executable, omitting from result.  Go pprofs will not be accurate.")

    # Echo the command line args used to run sgcollect_info
    cmd_line_args_task = AllOsTask(
        "Echo sgcollect_info cmd line args",
        "echo options: {0} args: {1}".format({k: ud(v, should_redact) for k, v in list(options.__dict__.items())}, args),
        log_file="sgcollect_info_options.log",
    )
    runner.run(cmd_line_args_task)

    runner.close_all_files()

    # Build redacted zip file
    if options.redact_level != "none":
        log("Redacting log files to level: %s" % options.redact_level)
        runner.redact_and_zip(redact_zip_file, 'sgcollect_info', options.salt_value, platform.node())

    # Build the actual zip file
    runner.zip(zip_filename, 'sgcollect_info', platform.node())

    # Upload the zip to the URL to S3 if required
    if upload_url:
        if options.redact_level != "none":
            do_upload_and_exit(redact_zip_file, upload_url, options.upload_proxy)
        else:
            do_upload_and_exit(zip_filename, upload_url, options.upload_proxy)

    if options.redact_level != "none":
        print("Zipfile built: {0}".format(redact_zip_file))

    print("Zipfile built: {0}".format(zip_filename))


def ud(value, should_redact=True):
    if not should_redact:
        return value
    return "<ud>{0}</ud>".format(value)


if __name__ == '__main__':
    main()
