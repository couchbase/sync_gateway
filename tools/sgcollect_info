#!/usr/bin/env python
# -*- python -*-
import os
import sys
import tempfile
import subprocess
import re
import platform
import glob
import optparse
import urllib2
import shutil
import json
import uuid

from sys import platform as _platform

from tasks import CbcollectInfoOptions
from tasks import dump_utilities
from tasks import generate_upload_url
from tasks import TaskRunner
from tasks import make_os_tasks
from tasks import log
from tasks import AllOsTask
from tasks import make_curl_task
from tasks import flatten
from tasks import do_upload_and_exit
from tasks import add_file_task
from tasks import add_gzip_file_task
from tasks import setup_stdin_watcher

import password_remover

import ssl
try:
    # Don't validate HTTPS by default.
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    # Running an older version of Python which won't validate HTTPS anyway.
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# Collects the following info from Sync Gateway
#
# - System Stats (top, netstat, etc)
# - Sync Gateway logs
# - Expvar Json
# - pprof files (profiling / memory)
# - Startup and running SG config
#
# See https://github.com/couchbase/sync_gateway/issues/1640
#
# Python version compatibility:
#
# Until the libc / centos6 issues are resolved, this should remain python 2.6 compatible.
# One common incompatibility is the formatting syntax, as discussed here: http://bit.ly/2rIH8wg
USAGE = """usage: %prog [options] output_file.zip

- Linux/Windows/OSX:
    %prog output_file.zip
    %prog -v output_file.zip"""

mydir = os.path.dirname(sys.argv[0])


def create_option_parser():
    parser = optparse.OptionParser(usage=USAGE, option_class=CbcollectInfoOptions)
    parser.add_option("-r", dest="root",
                      help="root directory - defaults to %s" % (mydir + "/.."),
                      default=os.path.abspath(os.path.join(mydir, "..")))
    parser.add_option("-v", dest="verbosity", help="increase verbosity level",
                      action="count", default=0)
    parser.add_option("-p", dest="product_only", help="gather only product related information",
                      action="store_true", default=False)
    parser.add_option("-d", action="callback", callback=dump_utilities,
                      help="dump a list of commands that sgcollect_info needs")
    parser.add_option("--watch-stdin", dest="watch_stdin",
                      action="store_true", default=False,
                      help=optparse.SUPPRESS_HELP)
    parser.add_option("--log-redaction-level", dest="redact_level",
                      default="none",
                      help="redaction level for the logs collected, none and partial supported (default is none)")
    parser.add_option("--log-redaction-salt", dest="salt_value",
                      default=str(uuid.uuid4()),
                      help="Is used to salt the hashing of tagged data, \
                            defaults to random uuid. If input by user it should \
                            be provided along with --log-redaction-level option")
    parser.add_option("--just-upload-into", dest="just_upload_into",
                      help=optparse.SUPPRESS_HELP)
    parser.add_option("--upload-host", dest="upload_host",
                      help="if specified, gathers diagnotics and uploads it to the specified host,"
                           " e.g 'https://s3.amazonaws.com/cb-customers'")
    parser.add_option("--customer", dest="upload_customer",
                      help="used in conjunction with '--upload-host' and '--ticket', "
                           "specifies the customer name for the upload")
    parser.add_option("--ticket", dest="upload_ticket", type='ticket',
                      help="used in conjunction with '--upload-host' and '--customer',"
                           " specifies the support ticket number for the upload."
                           " e.g 1234 (must be numeric), contact Couchbase Support to open a new"
                           "ticket if you do not already have one.  For more info, see"
                           "http://www.couchbase.com/wiki/display/couchbase/Working+with+the+Couchbase+Technical+Support+Team")
    parser.add_option("--sync-gateway-url", dest="sync_gateway_url",
                      help="Sync Gateway admin port URL, eg, http://localhost:4985")
    parser.add_option("--sync-gateway-config", dest="sync_gateway_config",
                      help="path to Sync Gateway config.  By default will try to discover via expvars")
    parser.add_option("--sync-gateway-executable", dest="sync_gateway_executable",
                      help="path to Sync Gateway executable.  By default will try to discover via expvars")
    parser.add_option("--upload-proxy", dest="upload_proxy", default="",
                      help="specifies proxy for upload")
    return parser


def expvar_url(sg_url):

    return '{0}/_expvar'.format(sg_url)


def make_http_client_pprof_tasks(sg_url):

    """
    These tasks use the python http client to collect the raw pprof data, which can later
    be rendered into something human readable
    """
    profile_types = [
        "profile",
        "heap",
        "goroutine",
        "block",
        "mutex",
    ]

    base_pprof_url = "{0}/_debug/pprof".format(sg_url)

    pprof_tasks = []
    for profile_type in profile_types:
        sg_pprof_url = "{0}/{1}".format(base_pprof_url, profile_type)
        clean_task = make_curl_task(name="Collect {0} pprof via http client".format(profile_type),
                                    user="",
                                    password="",
                                    url=sg_pprof_url,
                                    log_file="pprof_{0}.pb.gz".format(profile_type))
        clean_task.no_header = True
        pprof_tasks.append(clean_task)

    return pprof_tasks


def to_lower_case_keys_dict(original_dict):
    result = {}
    for k, v in original_dict.items():
        result[k.lower()] = v
    return result


def extract_element_from_config(element, config):
    """ The config returned from /_config may not be fully formed json
        due to the fact that the sync function is inside backticks (`)
        Therefore this method grabs an element from the config after
        removing the sync function
    """

    sync_regex = r'"Sync":(`.*`)'
    config = re.sub(sync_regex, '"Sync":""', config)
    try:
        # convert dictionary keys to lower case
        original_dict = json.loads(config)
        lower_case_keys_dict = to_lower_case_keys_dict(original_dict)

        # lookup key after converting element name to lower case
        return lower_case_keys_dict[element.lower()]
    except (ValueError, KeyError):
        # If we can't deserialise the json or find the key then return nothing
        return


def extract_element_from_default_logging_config(element, config):
    # extracts a property from nested logging object
        try:
            logging_config = extract_element_from_config('Logging', config)
            if logging_config:
                default_logging_config = extract_element_from_config('default', json.dumps(logging_config))
                if default_logging_config:
                    guessed_log_path = extract_element_from_config(element, json.dumps(default_logging_config))
                    if guessed_log_path:
                        return guessed_log_path
            return
        except (ValueError, KeyError):
            # If we can't deserialise the json or find the key then return nothing
            return


def extract_element_from_logging_config(element, config):
    # extracts a property from nested logging object
        try:
            logging_config = extract_element_from_config('logging', config)
            if logging_config:
                guessed_log_path = extract_element_from_config(element, json.dumps(logging_config))
                if guessed_log_path:
                    return guessed_log_path
            return
        except (ValueError, KeyError):
            # If we can't deserialise the json or find the key then return nothing
            return


def make_collect_logs_tasks(zip_dir, sg_url, salt):

    sg_log_files = {
        "sg_error.log": "sg_error.log",
        "sg_warn.log": "sg_warn.log",
        "sg_info.log": "sg_info.log",
        "sg_debug.log": "sg_debug.log",
        "sg_stats.log": "sg_stats.log",
        "sync_gateway_access.log": "sync_gateway_access.log",
        "sync_gateway_error.log": "sync_gateway_error.log",
        "sg_accel_access.log": "sg_accel_access.log",
        "sg_accel_error.log": "sg_accel_error.log",
    }

    os_home_dirs = [
        "/home/sg_accel/logs",
        "/home/sync_gateway/logs",
        "/var/log/sg_accel",
        "/var/log/sync_gateway",
        "/Users/sync_gateway/logs",                                                     # OSX sync gateway
        "/Users/sg_accel/logs",                                                         # OSX sg accel
        R'C:\Program Files (x86)\Couchbase\var\lib\couchbase\logs',                     # Windows (Pre-2.0)
        R'C:\Program Files\Couchbase\var\lib\couchbase\logs',                           # Windows (Post-2.0)
        R'C:\Program Files\Couchbase\Sync Gateway Accelerator\var\lib\couchbase\logs',  # Windows (Post-2.1) sg accel
        R'C:\Program Files\Couchbase\Sync Gateway\var\lib\couchbase\logs'               # Windows (Post-2.1) sync gateway
    ]
    # Try to find user-specified log path
    config_url = "{0}/_config".format(sg_url)
    try:
        response = urllib2.urlopen(config_url)
    except urllib2.URLError:
        config_str = ""
    else:
        config_str = str(response.read())

    # Find log file path from old style top level config
    guessed_log_path = extract_element_from_config('LogFilePath', config_str)
    if guessed_log_path:
        # Add the directory where the log file is so that it will be searched
        os_home_dirs.append(os.path.dirname(guessed_log_path))
        # Add the filename, in case it's different than sg_accel_error.log
        sg_log_files[os.path.basename(guessed_log_path)] = os.path.basename(guessed_log_path)

    # Keep a dictionary of log file paths we've added, to avoid adding duplicates
    sg_log_file_paths = {}

    sg_tasks = []

    for _, path in sg_log_files.iteritems():
        for os_home_dir in os_home_dirs:
            sg_log_file_path = os.path.join(os_home_dir, path)

            # As long as a task that monitors this log file path has not already been added, add a new task
            if sg_log_file_path not in sg_log_file_paths:
                task = add_file_task(sourcefile_path=sg_log_file_path)
                sg_tasks.append(task)

            # Track which log file paths have been added so far
            sg_log_file_paths[sg_log_file_path] = sg_log_file_path

    # Find log file path from logging.["default"] style config
    guessed_logging_path = extract_element_from_default_logging_config('LogFilePath', config_str)
    if guessed_logging_path:
        # Get the parent directory and the log file name
        log_file_parent_dir = os.path.abspath(os.path.join(guessed_logging_path, os.pardir))
        log_file_name = os.path.basename(guessed_logging_path)
        name, ext = os.path.splitext(log_file_name)

        # iterate over all log files, including those with a timestamp prefix
        log_file_pattern = "{0}*{1}".format(name, ext)
        rotated_logs_pattern = os.path.join(
           log_file_parent_dir,
           log_file_pattern
        )

        for log_file_item_name in glob.iglob(rotated_logs_pattern):
            log_file_item_path = os.path.join(log_file_parent_dir, log_file_item_name)
            # As long as a task that monitors this log file path has not already been added, add a new task
            if log_file_item_path not in sg_log_file_paths:
                print('Capturing rotated log file {0}'.format(log_file_item_path))
                task = add_file_task(sourcefile_path=log_file_item_path)
                sg_tasks.append(task)

            # Track which log file paths have been added so far
            sg_log_file_paths[log_file_item_path] = log_file_item_path

    # Find log file path from SGW 2.1 style logging config
    guessed_log_file_path = extract_element_from_logging_config('log_file_path', config_str)
    if guessed_log_file_path:
        log_file_path = os.path.abspath(guessed_log_file_path)

        for log_file_name in sg_log_files:
            # iterate over all log files, including those with a rotation timestamp
            # e.g: sg_info-2018-12-31T13-33-41.055.log
            name, ext = os.path.splitext(log_file_name)
            log_file_pattern = "{0}*{1}".format(name, ext)
            rotated_logs_pattern = os.path.join(
               log_file_path,
               log_file_pattern
            )

            for log_file_item_name in glob.iglob(rotated_logs_pattern):
                log_file_item_path = os.path.join(log_file_path, log_file_item_name)
                # As long as a task that monitors this log file path has not already been added, add a new task
                if log_file_item_path not in sg_log_file_paths:
                    print('Capturing rotated log file {0}'.format(log_file_item_path))
                    task = add_file_task(sourcefile_path=log_file_item_path)
                    sg_tasks.append(task)

                # Track which log file paths have been added so far
                sg_log_file_paths[log_file_item_path] = log_file_item_path

            # try gzipped logs too
            # e.g: sg_info-2018-12-31T13-33-41.055.log.gz
            log_file_pattern = "{0}*{1}.gz".format(name, ext)
            rotated_logs_pattern = os.path.join(
               log_file_path,
               log_file_pattern
            )

            for log_file_item_name in glob.iglob(rotated_logs_pattern):
                log_file_item_path = os.path.join(log_file_path, log_file_item_name)
                # As long as a task that monitors this log file path has not already been added, add a new task
                if log_file_item_path not in sg_log_file_paths:
                    print('Capturing compressed rotated log file {0}'.format(log_file_item_path))
                    task = add_gzip_file_task(sourcefile_path=log_file_item_path, salt=salt)
                    sg_tasks.append(task)

                # Track which log file paths have been added so far
                sg_log_file_paths[log_file_item_path] = log_file_item_path

    return sg_tasks


def get_db_list(sg_url):

    # build url to _all_dbs
    all_dbs_url = "{0}/_all_dbs".format(sg_url)
    data = []

    # get content and parse into json
    try:
        response = urllib2.urlopen(all_dbs_url)
        data = json.load(response)
    except urllib2.URLError as e:
        print("WARNING: Unable to connect to Sync Gateway: {0}".format(e))

    # return list of dbs
    return data


# Get the "status" for the server overall that's available
# at the server root URL, as well as the status of each database
def make_status_tasks(sg_url, should_redact):

    tasks = []

    # Get server config
    task = make_curl_task(name="Collect server status",
                          user="",
                          password="",
                          url=sg_url,
                          log_file="server_status.log")
    tasks.append(task)

    # Get list of dbs from _all_dbs
    # For each db, get db config
    dbs = get_db_list(sg_url)
    for db in dbs:
        db_status_url = "{0}/{1}".format(sg_url, db)
        task = make_curl_task(name="Collect db status for db: {0}".format(db),
                              user="",
                              password="",
                              url=db_status_url,
                              log_file="db_{0}_status.log".format(db))
        tasks.append(task)

    return tasks


# Startup config
#   Commandline args (covered in expvars, IIRC)
#   json file.
# Running config
#   Server config
#   Each DB config
def make_config_tasks(zip_dir, sg_config_path, sg_url, should_redact):

    collect_config_tasks = []

    # Here are the "usual suspects" to probe for finding the static config
    sg_config_files = [
        "/home/sg_accel/sg_accel.json",                                  # linux sg accel
        "/home/sync_gateway/sync_gateway.json",                          # linux sync gateway
        "/opt/sync_gateway/etc/sync_gateway.json",                       # amazon linux AMI sync gateway
        "/opt/sg_accel/etc/sg_accel.json",                               # amazon linux AMI sg accel
        "/Users/sync_gateway/sg_accel.json",                             # OSX sg accel old
        "/Users/sync_gateway/sync_gateway.json"                          # OSX sync gateway
        "/Users/sg_accel/sg_accel.json",                                 # OSX sg accel
        R'C:\Program Files (x86)\Couchbase\basic_sg_accel_config.json',  # Windows (Pre-2.0) sg accel
        R'C:\Program Files (x86)\Couchbase\serviceconfig.json'           # Windows (Pre-2.0) sync gateway
        R'C:\Program Files\Couchbase\Sync Gateway Accelerator\basic_sg_accel_config.json',  # Windows (Post-2.0) sg accel
        R'C:\Program Files\Couchbase\Sync Gateway\serviceconfig.json'    # Windows (Post-2.0) sync gateway
    ]
    sg_config_files = [x for x in sg_config_files if os.path.exists(x)]

    # If a config path was discovered from the expvars, or passed in via the user, add that in the
    # list of files to probe
    if sg_config_path is not None:
        sg_config_files.append(sg_config_path)

    # Tag user data before redaction, if redact_level is set
    server_config_postprocessors = [password_remover.remove_passwords]
    db_config_postprocessors = [password_remover.remove_passwords]
    if should_redact:
        server_config_postprocessors.append(password_remover.tag_userdata_in_server_config)
        db_config_postprocessors.append(password_remover.tag_userdata_in_db_config)

    # Get static server config
    for sg_config_file in sg_config_files:
        task = add_file_task(sourcefile_path=sg_config_file, content_postprocessors=server_config_postprocessors)
        collect_config_tasks.append(task)

    # Get server config
    server_config_url = "{0}/_config".format(sg_url)
    config_task = make_curl_task(name="Collect server config",
                                 user="",
                                 password="",
                                 url=server_config_url,
                                 log_file="running_server_config.log",
                                 content_postprocessors=server_config_postprocessors)
    collect_config_tasks.append(config_task)

    # Get list of dbs from _all_dbs
    # For each db, get db config
    dbs = get_db_list(sg_url)
    for db in dbs:
        db_config_url = "{0}/{1}/_config".format(sg_url, db)
        config_task = make_curl_task(name="Collect db config for db: {0}".format(db),
                                     user="",
                                     password="",
                                     url=db_config_url,
                                     log_file="running_db_{0}_config.log".format(db),
                                     content_postprocessors=db_config_postprocessors)
        collect_config_tasks.append(config_task)

    return collect_config_tasks


def get_config_path_from_cmdline(cmdline_args):

    for cmdline_arg in cmdline_args:
        # if it has .json in the path, assume it's a config file.
        # ignore any config files that are URL's for now, since
        # they won't be handled correctly.
        if ".json" in cmdline_arg and "http" not in cmdline_arg:
            return cmdline_arg
    return None


def get_paths_from_expvars(sg_url):

    data = None
    sg_binary_path = None
    sg_config_path = None

    # get content and parse into json
    try:
        response = urllib2.urlopen(expvar_url(sg_url))
        data = json.load(response)
    except urllib2.URLError as e:
        print("WARNING: Unable to connect to Sync Gateway: {0}".format(e))

    if data is not None and "cmdline" in data:
        cmdline_args = data["cmdline"]
        if len(cmdline_args) == 0:
            return (sg_binary_path, sg_config_path)
        sg_binary_path = cmdline_args[0]
        if len(cmdline_args) > 1:
            try:
                sg_config_path = get_absolute_path(get_config_path_from_cmdline(cmdline_args[1:]))
            except Exception as e:
                print("Exception trying to get absolute sync gateway path from expvars: {0}".format(e))
                sg_config_path = get_config_path_from_cmdline(cmdline_args[1:])

    return (sg_binary_path, sg_config_path)


def get_absolute_path(relative_path):
    sync_gateway_cwd = ''
    try:
        if _platform.startswith("linux"):
            sync_gateway_pid = subprocess.check_output(['pgrep', 'sync_gateway']).split()[0]
            sync_gateway_cwd = subprocess.check_output(['readlink', '-e', '/proc/{0}/cwd'.format(sync_gateway_pid)]).strip('\n')
    except subprocess.CalledProcessError:
        pass

    return os.path.join(sync_gateway_cwd, relative_path)


def make_download_expvars_task(sg_url):

    task = make_curl_task(
        name="download_sg_expvars",
        url=expvar_url(sg_url),
        log_file="expvars.json"
    )

    task.no_header = True

    return task


def make_sg_tasks(zip_dir, sg_url, sync_gateway_config_path_option, sync_gateway_executable_path, should_redact, salt):

    # Get path to sg binary (reliable) and config (not reliable)
    sg_binary_path, sg_config_path = get_paths_from_expvars(sg_url)
    print("Discovered from expvars: sg_binary_path={0} sg_config_path={1}".format(sg_binary_path, sg_config_path))

    # If user passed in a specific path to the SG binary, then use it
    if sync_gateway_executable_path is not None and len(sync_gateway_executable_path) > 0:
        if not os.path.exists(sync_gateway_executable_path):
            raise Exception("Path to sync gateway executable passed in does not exist: {0}".format(sync_gateway_executable_path))
        sg_binary_path = sync_gateway_executable_path

    # Collect logs
    collect_logs_tasks = make_collect_logs_tasks(zip_dir, sg_url, salt)

    py_expvar_task = make_download_expvars_task(sg_url)

    # If the user passed in a valid config path, then use that rather than what's in the expvars
    if sync_gateway_config_path_option is not None and len(sync_gateway_config_path_option) > 0 and os.path.exists(sync_gateway_config_path_option):
        sg_config_path = sync_gateway_config_path_option

    http_client_pprof_tasks = make_http_client_pprof_tasks(sg_url)

    # Add a task to collect Sync Gateway config
    config_tasks = make_config_tasks(zip_dir, sg_config_path, sg_url, should_redact)

    # Curl the / endpoint and /db endpoints and save output
    status_tasks = make_status_tasks(sg_url, should_redact)

    # Compbine all tasks into flattened list
    sg_tasks = flatten(
        [
            collect_logs_tasks,
            py_expvar_task,
            http_client_pprof_tasks,
            config_tasks,
            status_tasks,
        ]
    )

    return sg_tasks


def discover_sg_binary_path(options, sg_url):

    sg_bin_dirs = [
        "/opt/couchbase-sync-gateway/bin/sync_gateway",                      # Linux + OSX
        "/opt/couchbase-sg-accel/bin/sg_accel",                              # Linux + OSX
        R'C:\Program Files (x86)\Couchbase\sync_gateway.exe',                # Windows (Pre-2.0)
        R'C:\Program Files (x86)\Couchbase\sg_accel.exe',                    # Windows (Pre-2.0)
        R'C:\Program Files\Couchbase\Sync Gateway\sync_gateway.exe',         # Windows (Post-2.0)
        R'C:\Program Files\Couchbase\Sync Gateway Accelerator\sg_accel.exe', # Windows (Post-2.0)
    ]

    for sg_binary_path_candidate in sg_bin_dirs:
        if os.path.exists(sg_binary_path_candidate):
            return sg_binary_path_candidate

    sg_binary_path, _ = get_paths_from_expvars(sg_url)

    if options.sync_gateway_executable is not None and len(options.sync_gateway_executable) > 0:
        if not os.path.exists(options.sync_gateway_executable):
            raise Exception(
                "Path to sync gateway executable passed in does not exist: {0}".format(options.sync_gateway_executable))
        return sg_binary_path

    # fallback to whatever was specified in options
    return options.sync_gateway_executable


def main():

    # ask all tools to use C locale (MB-12050)
    os.environ['LANG'] = 'C'
    os.environ['LC_ALL'] = 'C'

    # Workaround MB-8239: erl script fails in OSX as it is unable to find COUCHBASE_TOP
    if platform.system() == 'Darwin':
        os.environ["COUCHBASE_TOP"] = os.path.abspath(os.path.join(mydir, ".."))

    # Parse command line options
    parser = create_option_parser()
    options, args = parser.parse_args()

    # Validate args
    if len(args) != 1:
        parser.error("incorrect number of arguments. Expecting filename to collect diagnostics into")

    # Setup stdin watcher if this option was passed
    if options.watch_stdin:
        setup_stdin_watcher()

    # Get the sg url the user passed in, or use the default
    sg_url = options.sync_gateway_url
    if sg_url is None:
        sg_url = "http://127.0.0.1:4985"
    print("Using Sync Gateway URL: {0}".format(sg_url))

    # Build path to zip directory, make sure it exists
    zip_filename = args[0]
    if zip_filename[-4:] != '.zip':
        zip_filename = zip_filename + '.zip'
    zip_dir = os.path.dirname(os.path.abspath(zip_filename))
    if not os.access(zip_dir, os.W_OK | os.X_OK):
        print("do not have write access to the directory %s" % (zip_dir))
        sys.exit(1)

    if options.redact_level != "none" and options.redact_level != "partial":
        parser.error("Invalid redaction level. Only 'none' and 'partial' are supported.")

    upload_url = ""
    should_redact = False
    if options.redact_level != "none":
        should_redact = True

        # Generate the s3 URL where zip files will be updated
        redact_zip_file = zip_filename[:-4] + "-redacted" + zip_filename[-4:]
        upload_url = generate_upload_url(parser, options, redact_zip_file)
    else:
        upload_url = generate_upload_url(parser, options, zip_filename)

    # Linux
    if os.name == 'posix':

        path = [
            mydir,
            '/opt/couchbase/bin',
            os.environ['PATH'],
            '/bin',
            '/sbin',
            '/usr/bin',
            '/usr/sbin'
        ]
        os.environ['PATH'] = ':'.join(path)

        library_path = [
            os.path.join(options.root, 'lib')
        ]

        current_library_path = os.environ.get('LD_LIBRARY_PATH')
        if current_library_path is not None:
            library_path.append(current_library_path)

        os.environ['LD_LIBRARY_PATH'] = ':'.join(library_path)

    # Windows
    elif os.name == 'nt':

        path = [
            mydir,
            os.environ['PATH']
        ]
        os.environ['PATH'] = ';'.join(path)

    # If user asked to just upload, then upload and exit
    if options.just_upload_into is not None:
        do_upload_and_exit(args[0], options.just_upload_into, options.upload_proxy)

    # Create a TaskRunner and run all of the OS tasks (collect top, netstat, etc)
    # The output of the tasks will go directly into couchbase.log
    runner = TaskRunner(verbosity=options.verbosity, default_name="sync_gateway.log")

    if not options.product_only:
        for task in make_os_tasks(["sync_gateway"]):
            runner.run(task)

    # Output the Python version if verbosity was enabled
    if options.verbosity:
        log("Python version: %s" % sys.version)

    # Find path to sg binary
    sg_binary_path = discover_sg_binary_path(options, sg_url)

    # Run SG specific tasks
    for task in make_sg_tasks(zip_dir, sg_url, options.sync_gateway_config, options.sync_gateway_executable, should_redact, options.salt_value):
        runner.run(task)

    if sg_binary_path is not None and sg_binary_path != "" and os.path.exists(sg_binary_path):
        runner.collect_file(sg_binary_path)
    else:
        print("WARNING: unable to find Sync Gateway executable, omitting from result.  Go pprofs will not be accurate.")

    # Echo the command line args used to run sgcollect_info
    cmd_line_args_task = AllOsTask(
        "Echo sgcollect_info cmd line args",
        "echo options: {0} args: {1}".format({k: ud(v, should_redact) for k, v in options.__dict__.items()}, args),
        log_file="sgcollect_info_options.log",
    )
    runner.run(cmd_line_args_task)

    runner.close_all_files()

    # Build redacted zip file
    if options.redact_level != "none":
        log("Redacting log files to level: %s" % options.redact_level)
        runner.redact_and_zip(redact_zip_file, 'sgcollect_info', options.salt_value, platform.node())

    # Build the actual zip file
    runner.zip(zip_filename, 'sgcollect_info', platform.node())

    # Upload the zip to the URL to S3 if required
    if upload_url:
        if options.redact_level != "none":
            do_upload_and_exit(redact_zip_file, upload_url, options.upload_proxy)
        else:
            do_upload_and_exit(zip_filename, upload_url, options.upload_proxy)

    if options.redact_level != "none":
        print("Zipfile built: {0}".format(redact_zip_file))

    print("Zipfile built: {0}".format(zip_filename))


def ud(value, should_redact=True):
    if not should_redact:
        return value
    return "<ud>{0}</ud>".format(value)


if __name__ == '__main__':
    main()
